{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a04baa14-d085-433e-b3b3-eb78354064fd",
   "metadata": {},
   "source": [
    "# Short Documentation of the Main Objects in This Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a4188a-09a3-4cd6-a0dd-9da036a6fd30",
   "metadata": {},
   "source": [
    "# `diffusion.py` : forward SDE & noise schedules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc43c25-5d6e-47a6-88d8-244abad4c539",
   "metadata": {},
   "source": [
    "## `forward_SDE` Class Documentation\n",
    "\n",
    "```python\n",
    "import torch\n",
    "class forward_sde: \n",
    "    def __init__(self, dimension, final_time, sigma_infty, device=torch.device('cpu')):\n",
    "\n",
    "        self.d = dimension\n",
    "        self.final_time = final_time\n",
    "        self.sigma_infty = sigma_infty\n",
    "        self.device = device\n",
    "        self.final = gaussian(dimension, \n",
    "                              torch.zeros(dimension, device = device), \n",
    "                              self.sigma_infty**2 * torch.eye(dimension, device = device))\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        self.final = self.final.to(device)\n",
    "```\n",
    "\n",
    "The `forward_sde` class provides a base implementation for stochastic differential equations (SDEs). It serves as a foundation for specific forward SDEs such as the Ornstein–Uhlenbeck process, also known as the **Variance-Preserving SDE (VPSDE)** .\n",
    "### **Attributes**\n",
    "\n",
    "| Attribute       | Type           | Description                                                                                         |\n",
    "|-----------------|----------------|-----------------------------------------------------------------------------------------------------|\n",
    "| `d`            | `int`          | Dimension of the state space.                                        |\n",
    "| `final_time`   | `float`        | Diffusion time $T$ of the SDE.                                       |\n",
    "| `sigma_infty`  | `float`        | Asymptotic standard deviation. |\n",
    "| `device`       | `torch.device`       | Computational device (e.g., `'cpu'` or `'cuda'`).                           |\n",
    "\n",
    "\n",
    "### **Methods**\n",
    "\n",
    "#### `__init__(self, dimension, final_time, sigma_infty, device=torch.device('cpu'))`\n",
    "\n",
    "Initializes the `forward_sde` class.\n",
    "\n",
    "#### `to(self, device)`\n",
    "\n",
    "Transfers the object to the specified computational device.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262010cb-8fda-4a71-9ec9-a3d175cffc4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## `forward_VPSDE` Class Documentation\n",
    "\n",
    "```python\n",
    "class forward_VPSDE(forward_sde):\n",
    "    def __init__(self, dimension, beta, sigma_infty, final_time, device=torch.device('cpu')):\n",
    "        super().__init__(dimension, final_time, sigma_infty, device)  \n",
    "        self.beta = beta\n",
    "        self.sigma_infty = sigma_infty\n",
    "    def alpha(self, time_t):\n",
    "        return self.beta(time_t) / (2 * self.sigma_infty**2) \n",
    "    def eta(self, time_t):\n",
    "        return torch.sqrt(self.beta(time_t))\n",
    "    def alpha_integrate(self, time_t): \n",
    "        return self.beta.integrate(time_t) / (2 * self.sigma_infty**2) \n",
    "    def sigma(self, time_t):\n",
    "        return self.sigma_infty * torch.sqrt(1. - torch.exp(- 2*self.alpha_integrate(time_t)))\n",
    "    def mu(self, time_t):\n",
    "        return torch.exp(-self.alpha_integrate(time_t))\n",
    "    def I1(self, time_t):\n",
    "        return 2*self.sigma_infty**2 * (torch.exp(self.alpha_integrate(time_t)) - 1.)\n",
    "    def I2(self, time_t):\n",
    "        return self.sigma_infty**2 * (torch.exp(2*self.alpha_integrate(time_t)) - 1.)\n",
    "```\n",
    "\n",
    "The `forward_VPSDE` class implements a **Variance-Preserving Stochastic Differential Equation (VPSDE)**. This process is commonly used in Score-based Generative Models, where the variance of the system evolves in a controlled manner throughout diffusion as opposed to **VESDE**.\n",
    "The class inherits from the `forward_sde` base class and introduces a scalar time-varying noise schedule $\\beta(t)$.\n",
    "\n",
    "### **Key Mathematical Characteristics**\n",
    "\n",
    "The **Variance-Preserving SDE** is defined as:\n",
    "$$\n",
    "\\mathrm{d} \\overrightarrow{X}_t = -\\frac{1}{2 \\sigma^2} \\beta(t) \\overrightarrow{X}_t \\, \\mathrm{d} t + \\sqrt{\\beta(t)} \\mathrm{d} B_t\n",
    "$$\n",
    "where:\n",
    "- $\\beta(t):[0,T] \\to \\mathbb{R}_+ $: time-dependent noise schedule.\n",
    "- $(B_t)_{t \\in [0,T]}$: standard Brownian motion in $\\mathbb{R}^d$.\n",
    "- $\\sigma^2$: variance of the the stationary distribution of the process.\n",
    "\n",
    "\n",
    "### **Attributes**\n",
    "\n",
    "| Attribute       | Type             | Description                                                                                       |\n",
    "|-----------------|------------------|---------------------------------------------------------------------------------------------------|\n",
    "| `beta`         | `callable`       | Time-dependent noise coefficient  $\\beta(t)$.                                                  |\n",
    "| Inherited      | From `forward_sde` | Attributes like `d`, `final_time`, `sigma_infty`, and `device`.                                  |\n",
    "\n",
    "### **Methods**\n",
    "\n",
    "| Method                       | Description                                                                                     |\n",
    "|------------------------------|-------------------------------------------------------------------------------------------------|\n",
    "| `alpha(self, time_t)`        | Returns the drift coefficient at time $t \\in [0,T]$.           |\n",
    "| `eta(self, time_t)`          | Returns the diffusion coefficient value at time $t \\in [0,T]$.                                   |\n",
    "| `alpha_integrate(self, time_t)` | Computes the integrated drift coefficient between time 0 and $t$.              |\n",
    "| `sigma(self, time_t)`        | Computes the standard deviation of the VPSDE process at time $t  \\in [0,T]$. |\n",
    "| `mu(self, time_t)`           | Computes the mean decay factor of the VPSDE process at time $t \\in [0,T]$.            |\n",
    "| `I1(self, time_t)`           | Computes integrated quantities for EI integrator scheme (see below for details). |\n",
    "| `I2(self, time_t)`           | Computes integrated quantities for EI integrator scheme (see below for details). |\n",
    "\n",
    "\n",
    "\n",
    "### **Detailed Method Descriptions**\n",
    "\n",
    "The class inherits from the `forward_sde` base class and introduces a time-varying noise schedule $\\beta(t) $. Here, $\\sigma_{\\infty}$ corresponds to the attribute `self.sigma_infty`, which represents the asymptotic standard deviation of the Gaussian stationary distribution of the OU process.\n",
    "\n",
    "#### `alpha(self, time_t)`\n",
    "\n",
    "**Description:**  \n",
    "Returns the drift coefficient value at time $t \\in [0,T]$:\n",
    "$$ \\alpha(t) = \\frac{\\beta(t)}{2\\sigma_{\\infty}^2}.\n",
    "$$\n",
    "**Parameters:**\n",
    "- `time_t` *(float or tensor)*: time $t$.\n",
    "\n",
    "**Returns:**\n",
    "- $\\alpha(t)$ *(float or tensor)*.\n",
    "\n",
    "---\n",
    "\n",
    "#### `eta(self, time_t)`\n",
    "\n",
    "**Description:**  \n",
    "Returns the diffusion coefficient value at time $t \\in [0,T]$:\n",
    "$$\n",
    "\\eta(t) = \\sqrt{\\beta(t)}.\n",
    "$$\n",
    "\n",
    "**Parameters:**\n",
    "- `time_t` *(float or tensor)*: time $t$.\n",
    "\n",
    "**Returns:**\n",
    "- $\\eta(t)$ *(float or tensor)*.\n",
    "\n",
    "---\n",
    "\n",
    "#### `alpha_integrate(self, time_t)`\n",
    "\n",
    "**Description:**  \n",
    "Computes the integrated drift coefficient between time 0 and $t$:  \n",
    "$$\n",
    "\\int_0^t \\alpha(t) \\, \\mathrm{d}s = \\frac{1}{2\\sigma_{\\infty}^2} \\int_0^t \\beta(s) \\, \\mathrm{d}s.\n",
    "$$\n",
    "\n",
    "**Parameters:**\n",
    "- `time_t` *(float or tensor)*: time $t$.\n",
    "\n",
    "**Returns:**\n",
    "- $\\int_0^t \\alpha(t) \\, \\mathrm{d}s)$ *(float or tensor)*.\n",
    "\n",
    "---\n",
    "\n",
    "#### `sigma(self, time_t)`\n",
    "\n",
    "**Description:**  \n",
    "Computes the standard deviation $\\sigma (t)$ of the VPSDE process at time $t  \\in [0,T]$:\n",
    "$$\n",
    "\\sigma(t) = \\sigma_{\\infty} \\sqrt{1 - \\exp\\left(-2 \\int_0^t \\alpha(s) \\, \\mathrm{d}s \\right)}\n",
    "$$\n",
    "\n",
    "**Parameters:**\n",
    "- `time_t` *(float or tensor)*: Time $t$.\n",
    "\n",
    "**Returns:**\n",
    "- $\\sigma(t)$ *(float or tensor)*: Standard deviation at time $t$.\n",
    "\n",
    "---\n",
    "\n",
    "#### `mu(self, time_t)`\n",
    "\n",
    "**Description:**  \n",
    "Computes the mean decay factor $\\mu(t)$ of the VPSDE process at time $t \\in [0,T]$. \n",
    "$$\n",
    "\\mu(t) = \\exp\\left(-\\int_0^t \\alpha(s) \\, \\mathrm{d}s \\right)\n",
    "$$\n",
    "\n",
    "**Parameters:**\n",
    "- `time_t` *(float or tensor)*: Time $t $.\n",
    "\n",
    "**Returns:**\n",
    "- $\\mu(t)$ *(float or tensor)*: Mean decay factor at time $t$.\n",
    "\n",
    "---\n",
    "\n",
    "#### `I1(self, time_t)`\n",
    "\n",
    "**Description:**  \n",
    "Computes the integral $I_1(t)$ (needed in the Exponential Integrator scheme):\n",
    "$$\n",
    "I_1(t) = 2\\sigma_{\\infty}^2 \\left(e^{\\int_0^t \\alpha(s) \\mathrm{d}s} - 1\\right).\n",
    "$$\n",
    "\n",
    "**Parameters:**\n",
    "- `time_t` *(float or tensor)*: time $t$.\n",
    "\n",
    "**Returns:**\n",
    "- $I_1(t)$ *(float or tensor)*.\n",
    "\n",
    "---\n",
    "\n",
    "#### `I2(self, time_t)`\n",
    "\n",
    "**Description:**  \n",
    "Computes the integral $I_2(t)$ (needed in the Exponential Integrator scheme):\n",
    "$$\n",
    "I_2(t) = \\sigma_{\\infty}^2 \\left(e^{2\\int_0^t \\alpha(s) \\mathrm{d}s} - 1\\right).\n",
    "$$\n",
    "\n",
    "**Parameters:**\n",
    "- `time_t` *(float or tensor)*: time $t$.\n",
    "\n",
    "**Returns:**\n",
    "- $I_2(t)$ *(float or tensor)*.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14495b7-c1f8-4812-83f8-a7d00227104b",
   "metadata": {},
   "source": [
    "## `generate_forward` Function Documentation\n",
    "\n",
    "```python\n",
    "def generate_forward(sde, x0, time_tau):\n",
    "    mean = sde.mu(time_tau) * x0\n",
    "    noise = sde.sigma(time_tau) * torch.randn_like(x0, device = sde.device)\n",
    "    return mean + noise, noise\n",
    "```\n",
    "\n",
    "\n",
    "The `generate_forward` function generates a sample from the **forward stochastic process**. Given an initial state $X_0$, it computes the forward step at a specified time $\\tau \\in [0,T]$. The function uses the **mean** and **noise** terms of the SDE to simulate the forward state.\n",
    "\n",
    "\n",
    "\n",
    "### **Mathematical Definition**\n",
    "\n",
    "Given an initial state $X_0 \\in \\mathbb{R}^d$, $Z \\sim \\mathcal{N}(0, I_d)$, $Z \\perp X_0$, the forward step at time $\\tau$ is given by:\n",
    "$$\n",
    "X_\\tau = \\mu(\\tau) x_0 + \\sigma(\\tau) Z.\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a5048-0d60-4b6c-ae24-b63f12e5c8e1",
   "metadata": {},
   "source": [
    "## `explicit_score` Class Documentation\n",
    "\n",
    "```python\n",
    "class explicit_score:\n",
    "    def __init__(self, sde, dataset):\n",
    "        self.mu_0, self.sigma_0 = dataset.mean_covar()\n",
    "        self.sde = sde\n",
    "        self.id_d = torch.eye(self.sde.d, device=self.sde.device)\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        if len(t) == 1:\n",
    "            t = torch.full((x.shape[0], 1), t.item(), device=self.sde.device)\n",
    "        mu_t, sigma_t = self.sde.mu(t), self.sde.sigma(t)\n",
    "        mat = torch.inverse(\n",
    "            (mu_t**2).unsqueeze(-1) * self.sigma_0\n",
    "            + (sigma_t**2).unsqueeze(-1) * self.id_d\n",
    "        )\n",
    "        score = -torch.bmm(mat, (x - mu_t * self.mu_0).unsqueeze(-1))\n",
    "        return score.squeeze(-1)\n",
    "```\n",
    "The `explicit_score` class provides a **closed-form (orcale) score function** for the forward process marginal $p_t(x)$ when the initial data distribution is given by $X_0 \\sim \\mathcal{N}(\\mu_0,\\Sigma_0)$ (see Lemma E.1 in the paper).\n",
    "\n",
    "### **Attributes**\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|------------|------|-------------|\n",
    "| `sde` | `forward_sde` or subclass | Provides the functions $\\mu(t)$, $\\sigma(t)$, the dimension `d`, and the computational `device`. |\n",
    "| `dataset` | object | Must implement `mean_covar()` returning `(mu_0, sigma_0)` with a mean vector `mu_0` of shape `(d,)` and a covariance matrix `sigma_0` of shape `(d, d)`. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a5617-9fc5-4aa3-9d92-ec9b17e7d7d6",
   "metadata": {},
   "source": [
    "## `loss_conditional` Class Documentation\n",
    "\n",
    "```python\n",
    "class loss_conditional:\n",
    "    def __init__(self, score_theta, sde, eps=1e-5):\n",
    "        self.score_theta = score_theta\n",
    "        self.sde = sde\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self, x0):\n",
    "        time_tau = torch.rand((x0.shape[0], 1), device=x0.device) * (self.sde.final_time - self.eps) + self.eps\n",
    "        x_tau, noise = generate_forward(self.sde, x0, time_tau)\n",
    "        score = self.sde.sigma(time_tau)**2 * self.score_theta(x_tau, time_tau)\n",
    "        target = -noise\n",
    "        loss = torch.mean(torch.sum((score - target)**2, axis=1))\n",
    "        return loss\n",
    "```\n",
    "\n",
    "\n",
    "The `loss_conditional` class defines the **conditional denoising score-matching loss** used to train the score network $s_\\theta : [0,T] \\times \\mathbb{R}^d \\to \\mathbb{R}^d$ in **Score-based Generative Models (SGMs)**.  \n",
    "\n",
    "\n",
    "\n",
    "### **Attributes**\n",
    "\n",
    "| Attribute | Type | Description |\n",
    "|------------|------|-------------|\n",
    "| `score_theta` | `callable` | Neural network approximating the score function $\\nabla_x \\log p_t(x) $. |\n",
    "| `sde` | `forward_sde` or subclass | Defines the forward diffusion process used to generate perturbed samples $X_\\tau$. |\n",
    "| `eps` | `float` | Small positive constant  for numerical stability to avoid sampling times too close to 0. |\n",
    "\n",
    "\n",
    "\n",
    "### **Methods**\n",
    "\n",
    "| Method | Description |\n",
    "|---------|-------------|\n",
    "| `__init__(self, score_theta, sde, eps=1e-5)` | Initializes the loss function with the given score model, SDE, and numerical cutoff. |\n",
    "| `__call__(self, x0)` | Computes the denoising score-matching loss for a batch of clean samples $ x_0 \\sim X_0$. |\n",
    "\n",
    "\n",
    "### **Mathematical Definition**\n",
    "\n",
    "At each call, a random time $\\tau \\sim \\mathcal{U}(\\varepsilon, T)$ independent of $X_0$ is sampled, and the noisy sample\n",
    "\n",
    "$$\n",
    "X_\\tau = \\mu(\\tau) X_0 + \\sigma(\\tau) Z, \\quad Z \\sim \\mathcal{N}(0, I_d),\n",
    "$$\n",
    "is generated. The objective minimized by `loss_conditional`, using a positive weighting function $\\lambda(\\tau) = \\sigma^2(\\tau)$ that controls the contribution of each diffusion time to the overall objective, yielding  \n",
    "$$\n",
    "\\mathcal{L}(\\theta)\n",
    "= \\mathbb{E}_{X_0, Z, \\tau} \\bigg[ \\big\\| \\sigma(\\tau)^2 s_\\theta(X_\\tau, \\tau) + Z \\big\\|^2 \\bigg].\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Returns**\n",
    "\n",
    "| Type | Description |\n",
    "|------|-------------|\n",
    "| `torch.Tensor` | Scalar loss value corresponding to the batch-averaged denoising score-matching objective. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54081b9-4a00-4672-a6db-8176a1e7679d",
   "metadata": {},
   "source": [
    "## Noise schedule choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21163062-888c-40cf-9fea-f32e38cba1ea",
   "metadata": {},
   "source": [
    "## `beta_parametric` Class Documentation\n",
    "\n",
    "```python\n",
    "class beta_parametric:\n",
    "    def __init__(self, a, final_time, beta_min, beta_max):\n",
    "        self.a = a\n",
    "        self.final_time = final_time\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        if a == 0:\n",
    "            self.delta = (beta_max - beta_min) / final_time\n",
    "        else: \n",
    "            self.delta = (beta_max - beta_min) / (math.exp(self.a * final_time) - 1.)\n",
    "    def __call__(self, t):\n",
    "        if np.abs(self.a) < 1e-3: #for numerical stability\n",
    "            return self.beta_min + self.delta * t\n",
    "        else:\n",
    "            return self.beta_min + self.delta * (torch.exp(self.a*t) - 1.)\n",
    "    def integrate(self, t): \n",
    "        if np.abs(self.a) < 1e-3:\n",
    "            return self.beta_min * t + 0.5 * self.delta * t**2\n",
    "        else:\n",
    "            return self.beta_min * t + self.delta * ((torch.exp(self.a*t)-1)/self.a - t)\n",
    "    def square_integrate(self,t):\n",
    "        if np.abs(self.a) < 1e-3:\n",
    "            return self.beta_min**2 * t +  self.beta_min * self.delta * t**2 + (1./3) * self.delta**2 * t**3  #modified\n",
    "        else:\n",
    "            res = self.beta_min**2 * t + 2*self.beta_min*self.delta*(torch.exp(self.a*t) / self.a - t) \n",
    "            res += (self.delta)**2 * ( (torch.exp(2*self.a*t))/(2* self.a) - 2* (torch.exp(self.a*t))/(self.a) + t)\n",
    "            res -= (2* self.beta_min * self.delta /self.a - self.delta**2 *(3/2)*(1/self.a))\n",
    "            return res  \n",
    "    def change_a(self, a): \n",
    "        self.a = a \n",
    "        if np.abs(self.a) < 1e-3: \n",
    "            self.delta = (self.beta_max - self.beta_min) / self.final_time \n",
    "        else:\n",
    "            self.delta = (self.beta_max - self.beta_min) / (math.exp(self.a * self.final_time) - 1.)\n",
    "```\n",
    "\n",
    "The `beta_parametric` class implements a parametric noise schedule $\\beta_a(t)\\in \\mathbb{R}_+$ for $a \\in \\mathbb{R}$,\n",
    "$$ \\beta_a(t) \\propto (\\mathrm{e}^{at} -1)/(\\mathrm{e}^{aT} -1).$$\n",
    "The parameter $a$, determines determines the convexity of the curve between its initial value $\\beta_{\\min}$ and its terminal value $\\beta_{\\max}$ over the interval $[0,T]$.Note in particular that it provides convex function when $a>0$, concave function when $a<0$ and linear function when $a=0$. In addition, it provides methods to compute integrals of $\\beta(t)$ or $\\beta^2(t)$. \n",
    "\n",
    "\n",
    " ## **Attributes**\n",
    "\n",
    "| Attribute       | Type    | Description                                                                                           |\n",
    "|-----------------|---------|-------------------------------------------------------------------------------------------------------|\n",
    "| `a`             | `float` | Determines whether the convexity of $\\beta_a(t)$.  |\n",
    "| `final_time`    | `float` | Diffusion time $T$.                                                                               |\n",
    "| `beta_min`      | `float` | Initial value of $\\beta_a$ (i.e. $\\beta(0)$ is diffusion starts at time $0$).                                                          |\n",
    "| `beta_max`      | `float` | Final value of $\\beta_a$ (i.e. $\\beta_a(T)$).                                                            |\n",
    "\n",
    "\n",
    "## **Methods**\n",
    "\n",
    " | Method                                | Description                                                                                                     |\n",
    "|---------------------------------------|-----------------------------------------------------------------------------------------------------------------|\n",
    "| `__init__(self, a, final_time, beta_min, beta_max)` | Initializes the schedule.                 |\n",
    "| `__call__(self, t)`                  | Returns  $\\beta_a(t)$ at a given time $t$.                                               |\n",
    "| `integrate(self, t)`                 | Computes $\\int_0^t \\beta_a(s) \\, \\mathrm{d} s$.                                                                           |\n",
    "| `square_integrate(self, t)`          | Computes $\\int_0^t \\beta_a^2(s) \\,   \\mathrm{d} s$.                                                                         |\n",
    "| `change_a(self, a)`                  | Updates the parameter `a`.                                                |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fd4b1-90da-404c-883e-c3f95af9788c",
   "metadata": {},
   "source": [
    "## `beta_cosine` Class Documentation\n",
    "\n",
    "```python\n",
    "class beta_cosine:\n",
    "    def __init__(self, final_time, beta_min, sigma2 = 1, clip_max = None):\n",
    "        self.final_time = final_time\n",
    "        self.beta_min = beta_min\n",
    "        self.clip_max = clip_max \n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, t):\n",
    "        sched_val = self.sigma2 * np.pi * np.tan(np.pi * (self.beta_min + t / self.final_time) / (2. * (self.beta_min + 1.))) / (self.final_time * (self.beta_min + 1.))\n",
    "        if self.clip_max is not None:\n",
    "            sched_val = min(sched_val, self.clip_max)\n",
    "        return sched_val\n",
    "\n",
    "     \n",
    "    def integrate(self, t):\n",
    "        h_t = np.cos(np.pi * (self.beta_min + t / self.final_time) / (2. * (self.beta_min + 1.)))**2.\n",
    "        h_0 = np.cos(np.pi * self.beta_min / (2 * (self.beta_min + 1)))**2\n",
    "        integral_beta = -2* self.sigma2 * np.log(h_t / h_0)\n",
    "        return integral_beta\n",
    "```\n",
    "\n",
    "The `beta_cosine` class implements a noise schedule $\\beta_{\\text{cos}}(t)$ based on a **cosine/tangent** relationship adapted from Improved Denoising Diffusion Probabilistic Models (Nichol & Dhariwal, 2021) to the VPSDE framework.\n",
    "     $$\n",
    "       \\beta_{\\text{cos}}(t) = \\sigma^2 \\frac{\\pi}{T \\times (\\beta_{\\min} + 1)} \\,\\tan\\!\\Bigl(\\frac{\\pi}{2}\\,\\frac{\\beta_{\\min} + \\tfrac{t}{T}}{\\beta_{\\min} + 1}\\Bigr).\n",
    "     $$\n",
    "\n",
    "\n",
    "## **Attributes**\n",
    "\n",
    "| Attribute     | Type      | Description                                                                               |\n",
    "|---------------|-----------|-------------------------------------------------------------------------------------------|\n",
    "| `final_time`  | `float`   | Diffusion time $T$.                                                                   |\n",
    "| `beta_min`    | `float`   | A parameter that shifts the cosine schedule.                                      |\n",
    "| `clip_max`    | `float` or `None` | Maximum allowed value for $\\beta(t)$. If `None`, no clipping is applied.                   |\n",
    "\n",
    "\n",
    "## **Methods**\n",
    "\n",
    "| Method                                    | Description                                                                         |\n",
    "|-------------------------------------------|-------------------------------------------------------------------------------------|\n",
    "| `__init__(self, final_time, beta_min, clip_max=None)` | Initializes the cosine-based noise schedule.                     |\n",
    "| `__call__(self, t)`                      | Returns the noise coefficient $\\beta_{\\text{cos}}(t)$ at time $t$.                           |\n",
    "| `integrate(self, t)`                     | Computes $\\int_0^t \\beta(s)\\, \\mathrm{d}s$.                                               |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe3ddf-b08c-4d98-b158-340c93f71301",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# `sampler.py` : discretization schemes for the backward process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c232f46-8ee5-4103-80b3-f3c6b62fbcbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## `Euler_Maruyama_discr_sampler` Function Documentation\n",
    "\n",
    "\n",
    "```python\n",
    "def Euler_Maruyama_discr_sampler(init, sde, score_theta, num_steps):\n",
    "    x_bar = init.clone()\n",
    "    step_size = sde.final_time / num_steps\n",
    "    with torch.no_grad():\n",
    "        time_T = torch.tensor([sde.final_time], dtype=init.dtype, device=init.device)\n",
    "        for n in range(1, num_steps+1):\n",
    "            rev_tn = time_T - n * step_size\n",
    "            drift = sde.alpha(rev_tn) * x_bar + sde.eta(rev_tn)**2 * score_theta(x_bar, rev_tn)\n",
    "            noise = sde.eta(rev_tn) * math.sqrt(step_size) * torch.randn_like(init)\n",
    "            x_bar += drift * step_size + noise\n",
    "    return x_bar\n",
    "```\n",
    "\n",
    "The `Euler_Maruyama_discr_sampler` function implements the **discrete-time Euler-Maruyama method** to approximate samples from the **reverse SDE** associated with forward SDE defined in the class `forward_sde`.\n",
    "\n",
    "\n",
    "\n",
    "### **Mathematical Definition**\n",
    "When the **forward SDE** is defined for $t \\in (0,T]$ as:\n",
    "\\begin{equation*}\n",
    "    \\operatorname{d}\\! \\overrightarrow{X}_t = - \\alpha(t) \\overrightarrow{X}_t \\operatorname{d}\\! t+ \\eta(t) \\operatorname{d}\\! B_t, \\quad \\overrightarrow{X}_0 \\sim \\pi\n",
    "\\end{equation*}\n",
    "with:\n",
    "- $ \\forall t \\in [0,T]$, $\\overrightarrow{X}_t \\in \\mathbb{R}^d$.\n",
    "- $\\alpha: [0,T] \\to [0, +\\infty)$: drift coefficient.\n",
    "- $\\eta: [0,T] \\to [0, +\\infty)$: diffusion coefficient (sometimes called volatility).\n",
    "- $(B_t)_{t \\in [0,T]}$: standard Brownian motion of $\\mathbb{R}^d$.\n",
    "\n",
    "The **reverse SDE** is defined as:\n",
    "\n",
    "$$\n",
    "d\\overleftarrow{X}_t = \\left[ \\alpha(T-t) \\overleftarrow{X}_t + \\eta^2(T-t)  \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\right] \\mathrm{d} t + \\eta(T-t) \\, \\mathrm{d} B_t, \n",
    "$$\n",
    "where:\n",
    "- for $x \\in \\mathbb{R}^d$ and $t \\in [0,T]$, $\\nabla \\log p_t(x) : \\mathbb{R}^d \\times [0,T] \\to \\mathbb{R}^d $ is the score function gradient of the log-likelihood or its parametric approximation $s_{\\theta}(x,t)$.\n",
    "\n",
    "The **Euler-Maruyama discretization** approximates this stochastic process by considering $N$ time intervals $t_1 =0 \\leq \\ldots \\leq t_k \\leq t_{k+1} \\leq \\ldots \\leq t_N = T$ such that $T = \\sum_{k = 1}^N \\Delta_k$ with $\\Delta_k = t_{k+1}-t_k$ so that\n",
    "\n",
    "$$\n",
    "\\bar{X}_{k+1} = \\bar{X}_k + \\Delta_k \\left[ \\alpha(T-t_k) \\bar{X}_k + \\eta^2(T-t_k) \\nabla \\log p_{T-t_k}( \\bar{X}_k) \\right]  + \\sqrt{\\Delta_k} \\eta(T-t_k)  Z_k,\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "- $Z_k \\sim \\mathcal{N}(0, I_d)$: an independent standard Gaussian noise.\n",
    "\n",
    "\n",
    "### **Parameters**\n",
    "\n",
    "| Parameter       | Type             | Description                                                                                       |\n",
    "|-----------------|------------------|---------------------------------------------------------------------------------------------------|\n",
    "| `init`         | `torch.Tensor`   | Initial state $\\bar{X}_0$.                                |\n",
    "| `sde`          | `object`         | SDE object with methods `alpha(t)` (drift), `eta(t)` (diffusion), and attribute `final_time`.     |\n",
    "| `score_theta`  | `callable`       | Score function $\\nabla \\log p_t(x)$ (or its approximation $s_\\theta(x, t)$).                       |\n",
    "| `num_steps`    | `int`            | Number of time discretization steps $N$ (in this implementation the step-size is contant).                                                     |\n",
    "\n",
    "\n",
    "\n",
    "### **Returns**\n",
    "\n",
    "- `torch.Tensor`: Final state $\\bar{X}_N$ approximating the solution to the reverse-time SDE.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d722b6d-5d99-407a-adce-9c22b469b72f",
   "metadata": {},
   "source": [
    "## `EI_discr_sampler` Function Documentation\n",
    "\n",
    "```python\n",
    "def EI_discr_sampler(init, sde, score_theta, num_steps):\n",
    "    x_bar = init.clone()\n",
    "    step_size = sde.final_time / num_steps\n",
    "    with torch.no_grad():\n",
    "        time_T = torch.tensor([sde.final_time], dtype=init.dtype, device=init.device)\n",
    "        for n in range(0, num_steps):\n",
    "            rev_tn = time_T - n * step_size\n",
    "            rev_tnp1 = rev_tn - step_size\n",
    "            delta = sde.alpha_integrate(rev_tn) - sde.alpha_integrate(rev_tnp1)\n",
    "            J1 = torch.exp(-sde.alpha_integrate(rev_tnp1)) * (sde.I1(rev_tn) - sde.I1(rev_tnp1))\n",
    "            J2 = torch.exp(-2*sde.alpha_integrate(rev_tnp1)) * (sde.I2(rev_tn) - sde.I2(rev_tnp1))\n",
    "            noise = torch.sqrt(J2) * torch.randn_like(init)\n",
    "            x_bar = torch.exp(delta) * x_bar + score_theta(x_bar, rev_tn) * J1 + noise \n",
    "    return x_bar\n",
    "```\n",
    "\n",
    "The `EI_discr_sampler` function implements an \"Exponential Integrator\" (EI) discretization scheme for the **reverse-time SDE** that makes use of integrated quantities derived from the SDE as defined in the appendix of the paper.\n",
    "\n",
    "\n",
    "### **Mathematical Definition**\n",
    "\n",
    "When the **forward SDE** is defined for $t \\in (0,T]$ as:\n",
    "\\begin{equation*}\n",
    "    \\operatorname{d}\\! \\overrightarrow{X}_t = - \\alpha(t) \\overrightarrow{X}_t \\operatorname{d}\\! t+ \\eta(t) \\operatorname{d}\\! B_t,\n",
    "\\end{equation*}\n",
    "where:\n",
    "- for all $t \\in [0,T]$, $\\overrightarrow{X}_t \\in \\mathbb{R}^d$.\n",
    "- $\\alpha: [0,T] \\to [0, +\\infty)$: drift coefficient.\n",
    "- $\\eta: [0,T] \\to [0, +\\infty)$: diffusion coefficient (sometimes called volatility).\n",
    "- $(B_t)_{t \\in [0,T]}$: standard Brownian motion of $\\mathbb{R}^d$.\n",
    "\n",
    "The **reverse SDE** is defined as:\n",
    "$$\n",
    "d\\overleftarrow{X}_t = \\left[ \\alpha(T-t) \\overleftarrow{X}_t + \\eta^2(T-t)  \\nabla \\log p_{T-t}(\\overleftarrow{X}_t) \\right] \\mathrm{d} t + \\eta(T-t) \\, \\mathrm{d} B_t, \n",
    "$$\n",
    "where:\n",
    "- for $x \\in \\mathbb{R}^d$ and $t \\in [0,T]$, $\\nabla \\log p_t(x) : \\mathbb{R}^d \\times [0,T] \\to \\mathbb{R}^d $ is the score function gradient of the log-likelihood or its parametric approximation $s_{\\theta}(x,t)$.\n",
    "\n",
    "The **Exponential Integrator discretization** scheme approximates this stochastic process by considering $N$ time intervals $t_1 =0 \\leq \\ldots \\leq t_k \\leq t_{k+1} \\leq \\ldots \\leq t_N = T$ such that $T = \\sum_{k = 1}^N \\Delta_k$ with $\\Delta_k = t_{k+1}-t_k$ so that\n",
    "\n",
    "$$\n",
    "\\bar{X}_{\n",
    "k+1} = \\mathrm{e}^{\\int^{T-t_k}_{T-t_{k+1}} \\alpha(t) \\mathrm{d} t} \\bar{X}_{k+1} + \\nabla \\log p_{T-t_k}( \\bar{X}_k) * \\mathrm{e}^{ - \\int_0^{T-t_{k+1}} \\alpha(t) \\mathrm{d} t} \\int_{T-t_{k+1}}^{T-t_k} \\mathrm{e}^{\\int_0^t \\alpha (t)} \\eta^2(t) \\mathrm{d} t + \\mathrm{e}^{ - \\int_0^{T-t_{k+1}} \\alpha(t) \\mathrm{d} t} \\sqrt{ \\int_{T-t_{k+1}}^{T-t_k} \\mathrm{e}^{2 \\int_0^t \\alpha (t)} \\eta^2(t) \\mathrm{d} t} Z_k\n",
    "$$\n",
    "where\n",
    "\n",
    "- $Z_k \\sim \\mathcal{N}(0, I_d)$: an independent standard Gaussian noise.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Parameters**\n",
    "\n",
    "| Parameter       | Type             | Description                                                                                 |\n",
    "|-----------------|------------------|---------------------------------------------------------------------------------------------|\n",
    "| `init`          | `torch.Tensor`   | Initial state $\\bar{X}_0$.                              |\n",
    "| `sde`           | `object`         | SDE object providing methods: `alpha_integrate(t)`, `I1(t)`, `I2(t)`, and attributes `final_time`, `device`. |\n",
    "| `score_theta`   | `callable`       | Score function $\\nabla \\log p_t(x)$ (or its approximation $s_\\theta(x, t)$).                 |\n",
    "| `num_steps`     | `int`            | Number of discretization steps $N$ (in this implementation the step-size is contant).                                                     |\n",
    "\n",
    "---\n",
    "\n",
    "### **Returns**\n",
    "\n",
    "- `torch.Tensor`: Final state $\\bar{X}_N$ approximating the solution to the reverse-time SDE.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc718a3-f782-4058-bdc3-ae015ddc3a15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# `functions.py` : empirical processing & distances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4cf531-92fd-4ef3-83d7-d97fb5171f83",
   "metadata": {},
   "source": [
    "## Empirical Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3acd8e-7c0a-444b-8585-ee8db0d76f41",
   "metadata": {},
   "source": [
    "### `normalize` Function Documentation\n",
    "\n",
    "```python\n",
    "def normalize(training_sample, rescale=1):\n",
    "    means = torch.mean(training_sample, dim=0)\n",
    "    std_devs = torch.std(training_sample, dim=0, unbiased=True)\n",
    "    normalized_sample = (training_sample - means) / (rescale * std_devs)\n",
    "    return normalized_sample, means, std_devs\n",
    "```\n",
    "\n",
    "### **Description**  \n",
    "Standardizes each feature of the input data (`training_sample`) by subtracting its mean and dividing by its (scaled) standard deviation. \n",
    "\n",
    "### **Mathematical Definition**\n",
    "\n",
    "Given a dataset $\\mathcal{D} = \\left( X_i \\right)_{i =1}^n \\in (\\mathbb{R}^d)^n$, its mean vector $\\mu$ and standard deviations $\\sigma$ for each dimension:\n",
    "\n",
    "$$\n",
    "\\mu_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij}, \n",
    "\\quad \n",
    "\\sigma_j = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_{ij} - \\mu_j)^2},\n",
    "$$\n",
    "\n",
    "the normalized data is:\n",
    "\n",
    "$$\n",
    "X_{ij}^{(\\mathrm{norm})} \n",
    "= \\frac{X_{ij} - \\mu_j}{\\text{rescale} \\times \\sigma_j}.\n",
    "$$\n",
    "\n",
    "### **Parameters**\n",
    "\n",
    "| Parameter          | Type             | Description                                                                    |\n",
    "|--------------------|------------------|--------------------------------------------------------------------------------|\n",
    "| `training_sample`  | `torch.Tensor`   | Input data of shape *(n, d)*, where *n* = number of samples and *d* = dimension. |\n",
    "| `rescale`          | `float`          | Optional rescaling factor (default = 1). |\n",
    "\n",
    "### **Returns**\n",
    "\n",
    "- **`normalized_sample`** *(torch.Tensor)*: The standardized data of shape *(n, d)*.  \n",
    "- **`means`** *(torch.Tensor)*: Vector of mean values for each dimension (shape *(d,)*).  \n",
    "- **`std_devs`** *(torch.Tensor)*: Vector of standard deviations for each dimension (shape *(d,)*).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6694e4e1-af9a-4564-994d-2bd22aba1361",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### `unnormalize` Function Documentation\n",
    "\n",
    "```python\n",
    "def unnormalize(normalized_sample, means, std_devs, rescale = 1):\n",
    "    original_sample = (normalized_sample * rescale * std_devs) + means\n",
    "    return original_sample\n",
    "```\n",
    "\n",
    "**Description:**  \n",
    "Reverts the standardization process of the function `normalize`.\n",
    "\n",
    "### **Parameters**\n",
    "\n",
    "| Parameter           | Type             | Description                                                        |\n",
    "|---------------------|------------------|--------------------------------------------------------------------|\n",
    "| `normalized_sample` | `torch.Tensor`   | Normalized data of shape *(n, d)*.                                 |\n",
    "| `means`             | `torch.Tensor`   | Vector of mean values of shape *(d,)*.                             |\n",
    "| `std_devs`          | `torch.Tensor`   | Vector of standard deviations of shape *(d,)*.                     |\n",
    "| `rescale`           | `float`          | Scaling factor used during normalization (default = 1.).            |\n",
    "\n",
    "### **Returns**\n",
    "\n",
    "- **`original_sample`** *(torch.Tensor)*: data recovered in the original scale.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfed2d3-b941-4733-b24a-b27a72bbf723",
   "metadata": {},
   "source": [
    "### `empirical_mean_covar` Function Documentation\n",
    "\n",
    "```python\n",
    "def empirical_mean_covar(sample):\n",
    "    mean = sample.mean(axis = 0)\n",
    "    sample_centered = sample - mean\n",
    "    covar = sample_centered.T @ sample_centered / (sample_centered.shape[0] - 1)\n",
    "    return mean, covar\n",
    "```\n",
    "\n",
    "Computes the empirical mean and covariance matrix.\n",
    "\n",
    "### **Parameters**\n",
    "\n",
    "| Parameter  | Type             | Description                                                                      |\n",
    "|------------|------------------|----------------------------------------------------------------------------------|\n",
    "| `sample`   | `torch.Tensor`   | Data of shape *(n, d)*, where *n* = number of samples and *d* = dimension.       |\n",
    "\n",
    "### **Returns**\n",
    "\n",
    "- **`mean`** *(torch.Tensor)*: Vector of shape *(d,)* representing the empirical mean.  \n",
    "- **`covar`** *(torch.Tensor)*: Matrix of shape *(d, d)* representing the empirical covariance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50530a99-eeef-4ef3-8e36-5a24c28cd6e8",
   "metadata": {},
   "source": [
    "## *Metrics & Distances*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf87fe5-0a6d-42ca-8299-ed853539c16c",
   "metadata": {},
   "source": [
    "### `relative_fisher_information_Gaussian` Function Documentation\n",
    "\n",
    "```python\n",
    "def relative_fisher_information_Gaussian(dataset, sde):\n",
    "    d = dataset.d\n",
    "    sigma_infty = sde.sigma_infty\n",
    "    mean, covar = dataset.mean_covar()\n",
    "    trace_covar = torch.trace(covar)\n",
    "    norm_mean = torch.sum(mean**2)\n",
    "    trace_inverse_covar = torch.trace(torch.inverse(covar))\n",
    "    result = (1/sigma_infty**4) * (trace_covar + norm_mean) \\\n",
    "             - (2*d/sigma_infty**2) + trace_inverse_covar\n",
    "    return result\n",
    "```\n",
    "\n",
    "Computes the **empirical relative Fisher Information** of a dataset of samples from Gaussian distribution $X_0 \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)$ and the stationnary distribution of a VPSDE process (as defined in `forward_VPSDE`) $X_{\\infty} \\sim \\mathcal{N}(0, \\sigma^2 \\mathrm{I}_d)$.\n",
    "\n",
    "### **Mathematical Definition**\n",
    "See Lemma E.4 in the Appendix of the paper.\n",
    "\n",
    "### **Parameters**\n",
    "\n",
    "| Parameter  | Type    | Description                                                      |\n",
    "|------------|---------|------------------------------------------------------------------|\n",
    "| `dataset`  | object  | An empirical object with a `mean_covar()` method returning *(mean, covar)*. |\n",
    "| `sde`      | object  | VPSDE object with attribute `sigma_infty`       |\n",
    "\n",
    "### **Returns**\n",
    "\n",
    "- **`result`** *(float or tensor)*: the empirical relative Fisher Information between $X_0$ and $X_{\\infty}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d81a40e-ca73-4177-89a8-b318766acb40",
   "metadata": {},
   "source": [
    "## `kl_divergence` Function Documentation\n",
    "\n",
    "```python\n",
    "def kl_divergence(mu1, sigma1, mu2, sigma2):\n",
    "    d = len(mu1)\n",
    "    delta_mu = mu2 - mu1\n",
    "    inverse_sigma2 = torch.pinverse(sigma2)\n",
    "    _, log_det_sigma1 = torch.linalg.slogdet(sigma1)\n",
    "    _, log_det_sigma2 = torch.linalg.slogdet(sigma2)\n",
    "    log_term = log_det_sigma2 - log_det_sigma1\n",
    "    trace_term = torch.trace(inverse_sigma2 @ sigma1)\n",
    "    delta_term = delta_mu @ inverse_sigma2 @ delta_mu[:, None]\n",
    "\n",
    "    return 0.5 * (log_term - d + trace_term + delta_term).item()\n",
    "```\n",
    "\n",
    "### **Description:**  \n",
    "Computes the **Kullback–Leibler divergence** between two multivariate Gaussian distributions: $\\mathcal{N}(\\mu_1, \\Sigma_1)$ and $\\mathcal{N}(\\mu_2, \\Sigma_2)$.\n",
    "\n",
    "### **Mathematical Definition:**\n",
    "See Lemma E.2 in the Appendix of the paper.\n",
    "\n",
    "\n",
    "### **Parameters**\n",
    "| Parameter   | Type             | Description                                           |\n",
    "|-------------|------------------|-------------------------------------------------------|\n",
    "| `mu1`       | `torch.Tensor`   | Mean of the first Gaussian, shape *(d,)*.            |\n",
    "| `sigma1`    | `torch.Tensor`   | Covariance of the first Gaussian, shape *(d, d)*.    |\n",
    "| `mu2`       | `torch.Tensor`   | Mean of the second Gaussian, shape *(d,)*.           |\n",
    "| `sigma2`    | `torch.Tensor`   | Covariance of the second Gaussian, shape *(d, d)*.   |\n",
    "\n",
    "### **Returns**\n",
    "\n",
    "- **`kl_value`** *(float)*: $\\mathrm{KL}\\bigl(\\mathcal{N}_1 \\|\\mathcal{N}_2\\bigr)$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d11c7-ade5-4133-9315-d73a3b1db145",
   "metadata": {},
   "source": [
    "## `wasserstein_w2` Function Documentation\n",
    "\n",
    "```python\n",
    "def wasserstein_w2(mu1, sigma1, mu2, sigma2):\n",
    "    mu1_np = mu1.cpu().numpy()\n",
    "    sigma1_np = sigma1.cpu().numpy()\n",
    "    mu2_np = mu2.cpu().numpy()\n",
    "    sigma2_np = sigma2.cpu().numpy()\n",
    "    diff_term = np.sum((mu1_np - mu2_np)**2)\n",
    "    sqrt_sigma1 = scipy.linalg.sqrtm(sigma1_np).real\n",
    "    sqrt_last = scipy.linalg.sqrtm(sqrt_sigma1 @ sigma2_np @ sqrt_sigma1).real\n",
    "    return math.sqrt((diff_term + np.trace(sigma1_np + sigma2_np - 2 * sqrt_last)).item())\n",
    "```\n",
    "\n",
    "**Description:**  \n",
    "Computes the **2-Wasserstein** distance between two multivariate Gaussian distributions: $\\mathcal{N}(\\mu_1, \\Sigma_1)$ and $\\mathcal{N}(\\mu_2, \\Sigma_2)$.\n",
    "\n",
    "### **Mathematical Definition**\n",
    "See Lemma E.3 in the Appendix of the paper.\n",
    "\n",
    "\n",
    "### **Parameters**\n",
    "\n",
    "| Parameter   | Type             | Description                                            |\n",
    "|-------------|------------------|--------------------------------------------------------|\n",
    "| `mu1`       | `torch.Tensor`   | Mean of the first Gaussian, shape *(d,)*.             |\n",
    "| `sigma1`    | `torch.Tensor`   | Covariance of the first Gaussian, shape *(d, d)*.     |\n",
    "| `mu2`       | `torch.Tensor`   | Mean of the second Gaussian, shape *(d,)*.            |\n",
    "| `sigma2`    | `torch.Tensor`   | Covariance of the second Gaussian, shape *(d, d)*.    |\n",
    "\n",
    "### **Returns**\n",
    "\n",
    "- **`w2_value`** *(float)*: $ \\mathcal{W}_2(\\mathcal{N}_1,\\mathcal{N}_2)$.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
